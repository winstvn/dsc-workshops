{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_Data_Pipelines",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf-Kbrm7mLxR"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUJJo13XCfQp"
      },
      "source": [
        "# import data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# import library to display plots/images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import tensorflow libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# import standard libraries to deal with files\n",
        "import pathlib\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A8MsUwb68mf"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg4uNrJpVx30"
      },
      "source": [
        "# create huge numpy array and save it to disk\n",
        "def make_huge_data(path: str) -> None:\n",
        "    huge_data_file = pathlib.Path(path)\n",
        "    huge_data_shape = (3000, 256, 256, 3)\n",
        "    huge_data_dtype = np.float64\n",
        "\n",
        "    if not huge_data_file.exists():\n",
        "        huge_data = np.random.rand(*huge_data_shape).astype(huge_data_dtype)\n",
        "        np.save(huge_data_file, huge_data)\n",
        "        del huge_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-JWniLkV99I"
      },
      "source": [
        "# setup the huge data file\n",
        "HUGE_DATA_FILE = \"/content/huge_data.npy\"\n",
        "make_huge_data(HUGE_DATA_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X53JnHhQ7DSM"
      },
      "source": [
        "# shows original and augmented image side by side\n",
        "def visualize_augmentation(original: tf.Tensor, augmented: tf.Tensor) -> None:\n",
        "    fig = plt.figure()\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.title('Original image')\n",
        "    plt.imshow(original)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.title('Augmented image')\n",
        "    plt.imshow(augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYXrBpTWKg7A"
      },
      "source": [
        "# returns a class name given the index number\n",
        "def get_class_name(info: tfds.core.dataset_info.DatasetInfo, \n",
        "                   class_num: int) -> str:\n",
        "    return info.features['label'].int2str(class_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2YzqZucvmqa"
      },
      "source": [
        "# Outline\n",
        "---\n",
        "1. Basics of `tf.data.Dataset`\n",
        "> 1.1 Methods for a manipulating `Dataset`\\\n",
        "> 1.2 Creating pipelines by chaining methods\\\n",
        "> 1.3 Working with a structured `Dataset`\n",
        "\n",
        "2. Creating `Dataset` from data in memory/disk\n",
        "> 2.1 `Dataset` from a NumPy array\\\n",
        "> 2.2 `Dataset` from a structured image directory\\\n",
        "> 2.3 `Dataset` from a CSV file\n",
        "\n",
        "3. Create a image classification model using data pipelines\n",
        "> 3.1 Load Dataset from TF Hub\\\n",
        "> 3.2 Build the pipeline\\\n",
        "> 3.3 Define the model\\\n",
        "> 3.4 Train the model\\\n",
        "> 3.5 Manually evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k55COsUouDg"
      },
      "source": [
        "# Basic Mechanics of TF Datasets\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRslHBi-s2CN"
      },
      "source": [
        "Let's first create a dataset from a normal python list of 10 numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aghtY2M7srk2"
      },
      "source": [
        "lst = list(range(10))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FXwbj_pt7Pt"
      },
      "source": [
        "If we look at the dataset object we just created, we can see that it says that shapes is () and the types are `int32`\n",
        "\n",
        "We'll see why shapes doesn't have any numbers in it shortly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4S-CBJktnMp"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFfKffSQubmO"
      },
      "source": [
        "Let's try to look at an element inside the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgUzj_hRuGPF"
      },
      "source": [
        "dataset[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ayYX6Xcv8Uc"
      },
      "source": [
        "Hmmm... That's interesting. Why can't we specify an element that we want to view in the dataset?\n",
        "\n",
        "Well, **datasets are actually iterable objects**, meaning that we can't actually access a specific element, like an array. If we want to view the contents of the dataset, we can either iterate over it using a `for` loop, or turn it into a python iterable object and use the `iter.next()` method on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZmo1OKLxOrM"
      },
      "source": [
        "# iterate over the dataset using for loop\n",
        "for x in dataset:\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me5hd6FK0dv-"
      },
      "source": [
        "Looking at the output above, we actually see that the shape of each tensor is (), or a scalar value. So looking back at the description of the dataset object, we see that the `shapes: ()` part is actually describing the shape of each item in the dataset.\n",
        "\n",
        "Also, if you're not familiar with tensors, they are essentially like numpy arrays or multidimensional arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX5MydFGxboe"
      },
      "source": [
        "# view the first element of the dataset\n",
        "next(iter(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lifS5YesxnHJ"
      },
      "source": [
        "We can also turn the `iter` object into a `list`, but this is highly unadvisable because we're defeating the purpose of the datasets, which is to not load the entire dataset into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxSH5SaN0Uwh"
      },
      "source": [
        "list(iter(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H16Vin6B3GjN"
      },
      "source": [
        "Okay, now that we're familiar with dataset objects, lets take a look at some methods that we can manipulate the datasets with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTmCpz5q36aY"
      },
      "source": [
        "## Manipulating Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45N9n_3V5ZZR"
      },
      "source": [
        "### Dataset.shuffle()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj_0VE-QERq2"
      },
      "source": [
        "First, we'll look at the shuffle method.\n",
        "Shuffling is an important part of the preprocessing steps, since we always want to make sure that our dataset is shuffled to not introduce bias in our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRN4-tj64Dza"
      },
      "source": [
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.Data.Dataset.shuffle(\n",
        "    buffer_size, \n",
        "    seed=None, \n",
        "    reshuffle_each_iteration=None\n",
        ")\n",
        "```\n",
        "\n",
        "- Randomly shuffles the elements of this dataset.\n",
        "\n",
        "- This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
        "\n",
        "- *We have to give an buffer size argument, which tells the dataset how many samples to use for the random shuffling*\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su00UZZP587I"
      },
      "source": [
        "list(iter(dataset.shuffle(10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4tkKZex6IEN"
      },
      "source": [
        "Okay, lets try shuffling with a `buffer_size` of 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmxSvMbx6xta"
      },
      "source": [
        "list(iter(dataset.shuffle(2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSMLMkGX7UxW"
      },
      "source": [
        "Above, we see that the a couple elements were shuffled, but the rest remained relatively in order. \n",
        "\n",
        "It's important to use specify a large enough `buffer_size`, but we also need to keep in mind that more memory is allocated with a larger `buffer_size`. So if our dataset is very large, we may not want to allocate the entire dataset to memory for shuffling. A typical recommended `buffer_size` is 1000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rEnlkUFMLL7"
      },
      "source": [
        "Though, having a `buffer_size` larger than the dataset doesn't allocate more memory. For instance, I can have a `buffer_size` of `200000` and it won't crash our colab instance since we're not actually putting any information in the extra buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCzZOKskM1n9"
      },
      "source": [
        "list(iter(dataset.shuffle(200000)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaC1rkpK8dtf"
      },
      "source": [
        "### Dataset.filter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PI54V7lADPp"
      },
      "source": [
        "The filter method is similar to python's `filter` method where we can filter the dataset according to a function returning a boolean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lGD21IEgo-"
      },
      "source": [
        "**TensorFlow docs description:**\\\n",
        "`tf.Data.Dataset.filter(predicate)`:\n",
        "\n",
        "- Filters this dataset according to `predicate`.\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JkG061_ACwe"
      },
      "source": [
        "# filter out odd numbers from dataset\n",
        "list(iter(dataset.filter(lambda x: x % 2 == 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ash4DLMnA3MD"
      },
      "source": [
        "### Dataset.map()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZX8d0J5A-O6"
      },
      "source": [
        "The `Dataset.map` method is similar to python's `map` method where we can map a function to each element in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uslYmFcZE50E"
      },
      "source": [
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.Data.Dataset.map(\n",
        "    map_func,\n",
        "    num_parallel_calls=None, \n",
        "    deterministic=None\n",
        ")\n",
        "```\n",
        "\n",
        "- Maps `map_func` across the elements of this dataset.\n",
        "\n",
        "- This transformation applies `map_func` to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. \n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnXJiTA6A5__"
      },
      "source": [
        "# multiply each element by 2\n",
        "list(iter(dataset.map(lambda x: x*2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK9YXTHhFRh0"
      },
      "source": [
        "We can also enable parallel processing by calling the `map` method with the `num_parallel_calls` argument.\n",
        "\n",
        "And, instead of specifying a number of parallel processes to the `num_parallel_calls` argument, we can pass `tf.data.AUTOTUNE` to let TensorFlow decide the optimal number of parallel calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57QJHrLdFQbF"
      },
      "source": [
        "# multiply each element by 2 using parallel processing\n",
        "list(iter(dataset.map(lambda x: x*2, num_parallel_calls=tf.data.AUTOTUNE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15NnAfUB-UOM"
      },
      "source": [
        "### Dataset.take()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6opk8pK_rIR"
      },
      "source": [
        "The `take` method allows us to construct a new dataset taking a specified number of elements from our original dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGSVT1pCGpED"
      },
      "source": [
        "**TensorFlow docs description:**\\\n",
        "`tf.Data.Dataset.take(count)`:\n",
        "- Creates a `Dataset` with at most `count` elements from this dataset.\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUZ5GQzq_-Y-"
      },
      "source": [
        "list(iter(dataset.take(3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqXmZhz4F_Kt"
      },
      "source": [
        "### Dataset.batch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTx_CQaVGDGZ"
      },
      "source": [
        "The `batch` method essentially groups together our dataset elements into a specified batch size. This is a really useful method for when we train our models, as batching essentially allows us to show a limited number of samples to the model every time the model is updated in the training phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AOa008VIG1q"
      },
      "source": [
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.Data.Dataset.batch(\n",
        "    batch_size, \n",
        "    drop_remainder=False, \n",
        "    num_parallel_calls=None, \n",
        "    deterministic=None\n",
        ")\n",
        "```\n",
        "- Combines consecutive elements of this dataset into batches.\n",
        "\n",
        "- The components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `False`).\n",
        "\n",
        "- If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `True` to prevent the smaller batch from being produced.\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BPNt54CF-Mh"
      },
      "source": [
        "list(iter(dataset.batch(3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKZB8pjgI8b3"
      },
      "source": [
        "We can see above that the shapes of the tensors are no longer empty because we turned them from scalars into a vector or array of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va_HqalsJL-Q"
      },
      "source": [
        "dataset.batch(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDouq5XQJT4H"
      },
      "source": [
        "Also, if we look at the shape of the dataset after we apply the `batch` method, we see that it has a shape of `(None,)`. `None` is essentially a placeholder value that shows that the shape of the tensors are indeterminant.\n",
        "\n",
        "When we set the `drop_remainder` argument to `True`, we see that we instead get a tensor shape of `batch_size` instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfNM9kWrJxaK"
      },
      "source": [
        "dataset.batch(3, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07s7H8TAU2pp"
      },
      "source": [
        "### Dataset.unbatch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoUuyAebU2px"
      },
      "source": [
        "The `unbatch` method undoes the batch operation and removes a dimension\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#unbatch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kywUtLH3VHZV"
      },
      "source": [
        "list(iter(dataset.batch(3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4JZRtT6VOBX"
      },
      "source": [
        "list(iter(dataset.batch(3).unbatch()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFkhrc4xbwzM"
      },
      "source": [
        "## Optimizing pipeline efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70GQvIrWb2TY"
      },
      "source": [
        "### Dataset.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zal6MBhe7P6"
      },
      "source": [
        "Caching allows us to cache a section of the pipeline to improve the performance of subsequent iterations on the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwwmCqs5XF3v"
      },
      "source": [
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.Data.Dataset.cache(\n",
        "    filename=''\n",
        ")\n",
        "```\n",
        "- Caches the elements in this dataset.\n",
        "\n",
        "- The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3EfJ9mCZrh8"
      },
      "source": [
        "list(iter(dataset.map(lambda x: x*2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrwhcqaqaRtV"
      },
      "source": [
        "list(iter(dataset.shuffle(10).cache()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0h7LjmJb6ZG"
      },
      "source": [
        "### Dataset.prefetch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtmXOtF-eNCN"
      },
      "source": [
        "Prefetching essentially allows us to specify the number of samples to prepare in the background while the current sample is being used to train a model. It's a vital component of data pipelines allowing us to improve the overall speed of training ML models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtUgCgCvdiir"
      },
      "source": [
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.Data.Dataset.prefetch(\n",
        "    buffer_size\n",
        ")\n",
        "```\n",
        "- Creates a Dataset that prefetches elements from this dataset.\n",
        "\n",
        "- **Most dataset input pipelines should end with a call to prefetch.** This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAZZ620A51wF"
      },
      "source": [
        "## Chaining methods (a.k.a data pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ShL0wmBSGc"
      },
      "source": [
        "Now that you're more familiar with datasets and their methods, let's look at how we can build pipelines from them.\n",
        "\n",
        "All of the methods shown above don't actually modify the original dataset; instead, they produce a new modified dataset. So we can use this to chain together methods and create a pipeline with our datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSK36gG1LOyM"
      },
      "source": [
        "pipeline = (dataset.map(lambda x: x + 1, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                   .filter(lambda x: x > 3)\n",
        "                   .cache()\n",
        "                   .shuffle(10)\n",
        "                   .batch(2, num_parallel_calls=tf.data.AUTOTUNE))\n",
        "                   .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "list(iter(pipeline))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TnUNkduN6pQ"
      },
      "source": [
        "The methods introduced above are only the tip of the iceberg, if you want to see all the available methods, visit the [Dataset documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#methods_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLTuIUtTNPoK"
      },
      "source": [
        "## Working with Structured Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qYvG14Pn4T"
      },
      "source": [
        "So far, we've only been looking at a dataset containing an array. But what if we have more than one array that we want to put in our dataset? \n",
        "\n",
        "Typically, we have the label and the features that we want to put into our datasets, so let's look at an example of how we do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1fq8_HzNO5n"
      },
      "source": [
        "features = np.random.rand(4, 3, 3)\n",
        "labels = np.random.rand(4)\n",
        "\n",
        "structured_ds = tf.data.Dataset.from_tensor_slices((labels, features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bLni-0VRLOh"
      },
      "source": [
        "structured_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73sxBUrDRSCp"
      },
      "source": [
        "Let's iterate over this dataset and see what we get"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCEBvnrZRPDS"
      },
      "source": [
        "for x in structured_ds:\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyqMnbNCTa0X"
      },
      "source": [
        "type(next(iter(structured_ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBrziz4kRinf"
      },
      "source": [
        "We see that the elements are tuples, which was the data structure that we passed when we constructed the dataset\n",
        "\n",
        "If we want to perform operations on this dataset, we need to unpack the tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brc7bv_CR5J-"
      },
      "source": [
        "for label, feature in structured_ds:\n",
        "    print(label.numpy(), feature.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO4X2B-eSGvU"
      },
      "source": [
        "And lets run a map on this structured dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-whcYGDSFcd"
      },
      "source": [
        "def map_fn(label, feature):\n",
        "    return label + 1, feature + 1\n",
        "\n",
        "list(iter(structured_ds.map(map_fn)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq20rGGMmaPn"
      },
      "source": [
        "# Create Datasets from memory/disk\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29OAfVaYBBl"
      },
      "source": [
        "## Create Dataset from NumPy Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2WsxJvKXZUm"
      },
      "source": [
        "file_byte_size = pathlib.Path(HUGE_DATA_FILE).stat().st_size\n",
        "print(f'{(file_byte_size/10**9):.2f}GB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYGkZBh0ebFX"
      },
      "source": [
        "It would take a long time load the data and would use a significant amount of memory. Instead, lets load the data using a memory mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxZUXxvIXHhm"
      },
      "source": [
        "del huge_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg4h0yXYdsKA"
      },
      "source": [
        "# load huge dataset using memmap\n",
        "huge_data = np.load(HUGE_DATA_FILE, mmap_mode='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWLxuoJ2V01L"
      },
      "source": [
        "Next, lets create labels for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAIa5ds1b7xX"
      },
      "source": [
        "# create labels for the huge dataset\n",
        "huge_data_labels = np.random.randint(0, 10, 3000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9IbpvDNhy-s"
      },
      "source": [
        "Now lets create a dataset from the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2DuCNluZ76C"
      },
      "source": [
        "huge_dataset = tf.data.Dataset.from_tensor_slices((huge_data_labels, huge_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnUxe20HhGc8"
      },
      "source": [
        "huge_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqHE-Pi-YCr6"
      },
      "source": [
        "next(iter(huge_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG9xiq_hX7C3"
      },
      "source": [
        "for label, data in huge_dataset.take(1):\n",
        "  print(label)\n",
        "  print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNymZPa6vo0i"
      },
      "source": [
        "## Create Dataset from structured image directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVi0jPRMRWRs"
      },
      "source": [
        "###Download the flowers directory from tensorflow hub\n",
        "\n",
        "[TF Datasets Catalog](https://www.tensorflow.org/datasets/catalog/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP3SZIxBQvwc"
      },
      "source": [
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.keras.utils.get_file(\n",
        "    fname, \n",
        "    origin, \n",
        "    untar=False,\n",
        "    md5_hash=None, \n",
        "    file_hash=None,\n",
        "    cache_subdir='datasets', \n",
        "    hash_algorithm='auto',\n",
        "    extract=False, \n",
        "    archive_format='auto', \n",
        "    cache_dir=None\n",
        ")\n",
        "```\n",
        "\n",
        "- Downloads a file from a URL if it not already in the cache.\n",
        "\n",
        "- By default the file at the url `origin` is downloaded to the `cache_dir ~/.keras`, placed in the cache_subdir `datasets`, and given the filename `fname`. The final location of a file `example.txt` would therefore be `~/.keras/datasets/example.txt`.\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfWYcV9WTaXL"
      },
      "source": [
        "flowers_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3eSq8pcPtvp"
      },
      "source": [
        "flowers_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVCm-m53TRV8"
      },
      "source": [
        "Let's move the downloaded directory into our workspace so we can examine it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frWEKSgVvZ5I"
      },
      "source": [
        "shutil.move(flowers_dir, '/content/flower_photos')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQgR-I5nv3hc"
      },
      "source": [
        "flowers_dir = '/content/flower_photos'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYtbW-tPPXMS"
      },
      "source": [
        "###Load Dataset from an image directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKDjBhtnPppc"
      },
      "source": [
        "`tf.keras.preprocessing.image_dataset_from_directory()`\n",
        "\n",
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory, \n",
        "    labels='inferred', \n",
        "    label_mode='int',\n",
        "    class_names=None, \n",
        "    color_mode='rgb', \n",
        "    batch_size=32, \n",
        "    image_size=(256,\n",
        "    256), \n",
        "    shuffle=True, \n",
        "    seed=None,\n",
        "    validation_split=None, \n",
        "    subset=None,\n",
        "    interpolation='bilinear', \n",
        "    follow_links=False, \n",
        "    smart_resize=False\n",
        ")\n",
        "```\n",
        "\n",
        "- Generates a `tf.data.Dataset` from image files in a directory.\n",
        "\n",
        "- If your directory structure is:\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_image_1.jpg\n",
        "......a_image_2.jpg\n",
        "...class_b/\n",
        "......b_image_1.jpg\n",
        "......b_image_2.jpg\n",
        "```\n",
        "\n",
        "- Then calling `image_dataset_from_directory(main_directory, labels='inferred')` will return a `Dataset` that yields batches of images from the subdirectories `class_a` and `class_b`, together with labels `0` and `1` (`0` corresponding to `class_a` and `1` corresponding to `class_b`).\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAJj4tz2w2No"
      },
      "source": [
        "flowers_dataset = tf.keras.preprocessing.image_dataset_from_directory(flowers_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avXmRpQ5S-za"
      },
      "source": [
        "next(iter(flowers_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5TIodhX1rUh"
      },
      "source": [
        "next(iter(flowers_dataset.unbatch()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufLlhHZkm7UK"
      },
      "source": [
        "## Create Dataset from CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYku9PJGgaNI"
      },
      "source": [
        "### Create a Dataset from a CSV loaded into a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CqUmhgS0lnQ"
      },
      "source": [
        "housing_csv_file = '/content/sample_data/california_housing_train.csv'\n",
        "df = pd.read_csv(housing_csv_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW0do1lzq8gy"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrVh4gvFn9O9"
      },
      "source": [
        "housing_labels = df.loc[:, 'median_house_value']\n",
        "housing_features = df.drop(columns='median_house_value')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYW9c8z8XrNX"
      },
      "source": [
        "housing_labels.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ4z0a-ZrWJ9"
      },
      "source": [
        "housing_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHx2pdT4rd_w"
      },
      "source": [
        "housing_dataset = tf.data.Dataset.from_tensor_slices((housing_labels.values, housing_features.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MQZu739gcFR"
      },
      "source": [
        "### Create a Dataset directly from the CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7P0O-evf034"
      },
      "source": [
        "**TensorFlow docs description:**\n",
        "```\n",
        "tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern, \n",
        "    batch_size, \n",
        "    column_names=None, \n",
        "    column_defaults=None,\n",
        "    label_name=None, \n",
        "    select_columns=None, \n",
        "    field_delim=',',\n",
        "    use_quote_delim=True, \n",
        "    na_value='', \n",
        "    header=True, \n",
        "    num_epochs=None,\n",
        "    shuffle=True, \n",
        "    shuffle_buffer_size=10000, \n",
        "    shuffle_seed=None,\n",
        "    prefetch_buffer_size=None, \n",
        "    num_parallel_reads=None, \n",
        "    sloppy=False,\n",
        "    num_rows_for_inference=100, \n",
        "    compression_type=None, \n",
        "    ignore_errors=False\n",
        ")\n",
        "```\n",
        "\n",
        "- Reads CSV files into a dataset, where each element of the dataset is a (features, labels) tuple that corresponds to a batch of CSV rows. The features dictionary maps feature column names to `Tensor`s containing the corresponding feature data, and labels is a `Tensor` containing the batch's label data.\n",
        "\n",
        "[Documentation](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9wVqcBGsXft"
      },
      "source": [
        "housing_dataset = tf.data.experimental.make_csv_dataset(housing_csv_file, label_name='median_house_value', batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysURMogjtBNF"
      },
      "source": [
        "next(iter(housing_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMzh0i6ghPkj"
      },
      "source": [
        "next(iter(housing_dataset.unbatch()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkCsPLeZ-XEP"
      },
      "source": [
        "# Full workflow example\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYh3SQyRYV8H"
      },
      "source": [
        "## Load Dataset from TensorFlow Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTWsAdhMiQJG"
      },
      "source": [
        "**Load the `tf_flowers` dataset from the [TensorFlow Dataset catalog](https://www.tensorflow.org/datasets/catalog/overview)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8EnUFFYyzxf"
      },
      "source": [
        "(train_ds, val_ds), info = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:80%]', 'train[80%:]'],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtZklJc6Or1E"
      },
      "source": [
        "tfds.show_examples(train_ds.take(3), info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnEHfzZmBfb"
      },
      "source": [
        "info.features['label'].int2str(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOwv_0f68suD"
      },
      "source": [
        "## Build Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsUOBSVkCSL"
      },
      "source": [
        "Create a function for resizing and rescaling the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HujzUCY0ZdRi"
      },
      "source": [
        "def resize_and_rescale(image, label, size):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = tf.image.resize(image, size)\n",
        "    image = image / 255.0\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0AJj87lkEZn"
      },
      "source": [
        "Visualize the function results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZQO1BKci_1T"
      },
      "source": [
        "img, label = next(iter(train_ds))\n",
        "\n",
        "resized_img, _ = resize_and_rescale(img, label, (256, 256))\n",
        "\n",
        "visualize_augmentation(img, resized_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iBooOd5kKH6"
      },
      "source": [
        "Create a function for augmentating the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78X0sR7ocw0H"
      },
      "source": [
        "def img_augmentations(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_saturation(image, 1, 2)\n",
        "    image = tf.image.random_brightness(image, 0.1)\n",
        "    image = tf.image.random_hue(image, 0.1)\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBKdefuEkO-k"
      },
      "source": [
        "Create our pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF8Za-ANZXmo"
      },
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "pipeline = (train_ds\n",
        "                .map(lambda img, label: resize_and_rescale(img, label, IMG_SIZE), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .map(img_augmentations, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SweKcMu7kRgf"
      },
      "source": [
        "Visualize the pipeline results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1LLwfIJi06Z"
      },
      "source": [
        "def show(image, label):\n",
        "    plt.imshow(image)\n",
        "    plt.title(str(label.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PAmVn40ibt2"
      },
      "source": [
        "show(*next(iter(pipeline)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuqwn-adnNgU"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKELx3vbkrQp"
      },
      "source": [
        "We will be using the [MobileNet V3 Small](https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5) (input image size must be 224x224)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwSyYCBonLS9"
      },
      "source": [
        "model_handle = 'https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcfBzP8dtBFY"
      },
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "input_shape = IMG_SIZE + (3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_IV1g4Dq3Cb"
      },
      "source": [
        "num_classes = info.features['label'].num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ooOsRioTtk"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "    hub.KerasLayer(model_handle),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6g4H80orAiv"
      },
      "source": [
        "model.build((None,)+input_shape)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTGmcEEjrMyd"
      },
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), \n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgdc1ASXqQJT"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "train_ds_size = train_ds.cardinality().numpy()\n",
        "val_ds_size = val_ds.cardinality().numpy()\n",
        "\n",
        "train_processed = (train_ds\n",
        "                .map(lambda img, label: resize_and_rescale(img, label, IMG_SIZE), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .cache()\n",
        "                .map(img_augmentations, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                .shuffle(train_ds_size)\n",
        "                .batch(BATCH_SIZE)\n",
        "                .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_processed = (val_ds\n",
        "                 .map(lambda img, label: resize_and_rescale(img, label, IMG_SIZE), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                 .cache()\n",
        "                 .shuffle(val_ds_size)\n",
        "                 .batch(BATCH_SIZE)\n",
        "                 .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM8r78Zi-SBw"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QhiTQDMrYBS"
      },
      "source": [
        "hist = model.fit(\n",
        "    train_processed,\n",
        "    validation_data=val_processed,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=10,\n",
        "    validation_steps=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8VkQM_N-nZh"
      },
      "source": [
        "## Create Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQmQ1YoiHfoZ"
      },
      "source": [
        "test_img, test_label = next(iter(val_processed.unbatch().take(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAPmRHrx0RIU"
      },
      "source": [
        "prediction = model.predict(np.expand_dims(test_img, axis=0))\n",
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWtp_mitHnAx"
      },
      "source": [
        "plt.imshow(test_img)\n",
        "plt.title(get_class_name(info, prediction.argmax()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVQx87rObbPY"
      },
      "source": [
        "make a function to compare prediction results on 1 batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNpKcTphIdy0"
      },
      "source": [
        "def show_predictions(processed_val_dataset, info):\n",
        "    processed_val_dataset = processed_val_dataset.unbatch()\n",
        "\n",
        "    for test_img, test_label in processed_val_dataset:\n",
        "        plt.figure()\n",
        "        plt.axis('off')\n",
        "        plt.imshow(test_img)\n",
        "        prediction = model.predict(np.expand_dims(test_img, axis=0))\n",
        "        prediction_label = get_class_name(info, prediction.argmax()) \n",
        "        actual_label = get_class_name(info, test_label.numpy())\n",
        "        plt.title(f'Predicted: {prediction_label}, Actual: {actual_label}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUbkd5UK0viZ"
      },
      "source": [
        "show_predictions(val_processed.take(1), info)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}